# Notable P(doom) Estimates

**Comprehensive survey of expert predictions for AI existential risk**

---

## Estimate Distribution

### Ultra-High Risk (>80%)
Researchers who believe catastrophic AI outcomes are overwhelmingly likely

| Name | Estimate | Role | Notes |
|------|----------|------|-------|
| **Roman Yampolskiy** | 99.999999% | AI safety scientist, U of Louisville | Within 100 years |
| **Eliezer Yudkowsky** | >95% | MIRI founder, rationalist leader | Alignment fundamentally unsolved |
| **Max Tegmark** | >90% | Physicist, Future of Life Institute | Mathematical universe hypothesis creator |
| **Connor Leahy** | 90%+ | EleutherAI cofounder | Open-source AI researcher |
| **Andrew Critch** | 85% | CFAR founder | AI safety theorist |
| **David Duvenaud** | 85% | Former Anthropic Safety Team Lead | Inside view from leading lab |
| **Dan Hendrycks** | >80% | Center for AI Safety director | Up from ~20% two years prior |

### High Risk (50-80%)
Researchers who consider doom more likely than not

| Name | Estimate | Role | Notes |
|------|----------|------|-------|
| **Daniel Kokotajlo** | 70-80% | AI researcher, formerly OpenAI | Founded AI Futures Project |
| **Zvi Mowshowitz** | 60% | AI writer, CFAR board member | Rationalist community figure |
| **Yoshua Bengio** | 50% | MILA director, Nobel laureate 2024 | "Godfather of AI", one of big three |
| **Paul Christiano** | 50% (46% in another survey) | US AI Safety Institute head | Former OpenAI alignment lead |
| **Holden Karnofsky** | 50% | Open Philanthropy executive | Major AI safety funder |
| **Emad Mostaque** | 50% | Stability AI cofounder | Generative AI entrepreneur |

### Moderate Risk (10-50%)
Researchers who see serious but non-majority probability

| Name | Estimate | Role | Notes |
|------|----------|------|-------|
| **Scott Alexander** | 33% | Blogger (Astral Codex Ten) | Influential rationalist |
| **Survey of 44 AI safety researchers** | 30% (mean) | 2021 survey | Specialized in alignment |
| **Dario Amodei** | 25% | Anthropic CEO | "Things go really, really badly" |
| **Yoshua Bengio (alt estimate)** | 20% | MILA director | Lower estimate from different source |
| **Lina Khan** | ~15% | Former FTC chair | Policy perspective |
| **Vitalik Buterin** | 12% | Ethereum cofounder | Crypto/tech perspective |
| **Geoffrey Hinton** | 10-20% (all-things-considered)<br>>50% (independent impression) | "Godfather of AI", Nobel 2024 | Conditional vs unconditional |
| **Toby Ord** | 10% | Philosopher, *The Precipice* author | This century |
| **Elon Musk** | 10-30% | CEO Tesla/SpaceX/X | "But we should do it anyway" |
| **Lex Fridman** | 10% | AI researcher, podcaster | |
| **Jan Leike** | 10-90% | Anthropic (ex-OpenAI/DeepMind) | Wide uncertainty range |

### Low Risk (1-10%)
Researchers who see AI risk as real but unlikely

| Name | Estimate | Role | Notes |
|------|----------|------|-------|
| **Emmett Shear** | 5-50% | Twitch cofounder, ex-OpenAI interim CEO | Wide range |
| **Shane Legg** | 5-50% | Google DeepMind cofounder | Chief AGI Scientist |
| **Casey Newton** | 5% | Technology journalist | |
| **Nate Silver** | 5-10% | Statistician, FiveThirtyEight founder | |
| **AI Researcher Survey (2023)** | 5% (median), 14.4% (mean) | Thousands of researchers | Most comprehensive survey |
| **Eli Lifland** | c. 35-40% | Top superforecaster, AI 2027 co-author | Calibrated prediction |
| **Demis Hassabis** | Greater than 0% | DeepMind CEO, Nobel 2024 | Deliberately non-specific |

### Minimal Risk (<1%)
Researchers who dismiss or heavily discount AI existential risk

| Name | Estimate | Role | Notes |
|------|----------|------|-------|
| **Forecasting Research Institute Superforecasters** | 0.38% | Calibrated forecasting team | Track record of accuracy |
| **Benjamin Mann** | 0-10% | Anthropic cofounder | Inside view from safety-focused lab |
| **Yann LeCun** | <0.01% | Meta Chief AI Scientist, Turing Award | "Less likely than asteroid" |
| **Grady Booch** | ~0% | Software engineer | "Like oxygen spontaneously moving to corner" |
| **Marc Andreessen** | 0% | VC, Andreessen Horowitz | E/acc philosophy |

---

## Survey Data

### AI Impacts 2023 Survey
**Largest systematic survey of AI researchers**

- **Respondents:** Thousands of AI researchers
- **Question:** Probability of human extinction or similarly severe outcomes within next 100 years
- **Results:**
  - **Mean:** 14.4%
  - **Median:** 5%
  - **Distribution:** Wide variance, some extremely high estimates pull mean above median

### 2021 AI Safety Researchers Survey
**Specialized survey of alignment-focused researchers**

- **Respondents:** 44 AI safety researchers
- **Mean:** 30%
- **Note:** Higher than general AI researchers, suggests those studying alignment are more pessimistic

---

## Temporal Trends

### Pre-2020: Low Baseline
- Typical estimates: 1-5%
- Limited mainstream concern
- Niche topic in rationalist community

### 2020-2023: Sharp Increase
- **GPT-3 (June 2020):** Demonstrated scaling laws
- **GPT-4 (March 2023):** Professional-level capabilities
- Notable shift: Geoffrey Hinton, Yoshua Bengio warn publicly
- Estimates rise to 10-20% range for moderates

### 2023-2025: Continued Escalation
- **Statement on AI Risk of Extinction (May 2023):** Hundreds of researchers sign
- **o1 release (Sept 2024):** Reasoning capabilities surprise researchers
- **GPT-5 (Aug 2025):** Frontier model at 10²⁶+ FLOPS scale
- Current median: ~12% (Wikipedia data)

### Individual Changes Over Time
- **Dan Hendrycks:** ~20% → >80% (2 years)
- Reflects rapid capability gains and alignment difficulty

---

## Factors Influencing Estimates

### Technical Factors
1. **AGI timeline beliefs**
   - Shorter timelines → higher P(doom)
   - Those expecting AGI by 2027-2030 typically >30%

2. **Alignment difficulty**
   - Belief alignment is "fundamentally hard" → higher estimates
   - Optimism about technical solutions → lower estimates

3. **Takeoff speed**
   - Fast takeoff scenarios → higher P(doom)
   - Gradual development → more time for safety work

### Epistemic Factors
1. **Inside vs. outside view**
   - **Inside view:** Technical analysis of AI systems
   - **Outside view:** Reference class forecasting (nuclear, bio)

2. **Researcher affiliation**
   - Safety researchers tend higher (mean 30%)
   - Capabilities researchers tend lower
   - Industry leaders vary widely

3. **Calibration**
   - Superforecasters (trained calibration): 0.38%
   - Theorists (speculative reasoning): often >50%

---

## Conditional vs. Unconditional

### Geoffrey Hinton's Distinction
- **Independent impression (conditional on AGI):** >50%
- **All-things-considered (accounting for uncertainty):** 10-20%

This distinction matters because:
1. P(doom | AGI developed) may be high
2. But P(AGI developed) introduces uncertainty
3. Product gives lower unconditional P(doom)

### Typical Conditionals
Most estimates implicitly condition on:
- AGI is developed (excluding timelines where it never happens)
- Current trajectory continues (no major slowdown)
- 100-year time horizon (some specify "this century")

---

## Criticisms and Limitations

### Definitional Ambiguity
1. **What is "doom"?**
   - Extinction vs. permanent disempowerment
   - Partial vs. total catastrophe

2. **What time frame?**
   - 10 years, 50 years, 100 years?
   - Conditional on AGI arrival?

3. **What probability type?**
   - Bayesian credence vs. frequentist
   - One-time event → no empirical validation

### Methodological Concerns
1. **Anchoring bias:** Estimates may anchor on others' numbers
2. **Availability heuristic:** Recent AI advances overweight risk
3. **Groupthink:** Rationalist community may reinforce high estimates
4. **Career incentives:** Safety researchers benefit from high perceived risk

### Counterarguments
1. **Track records:** Some high estimators (Yudkowsky) predicted AI capabilities correctly
2. **Expert consensus:** Median 5-14% is non-trivial even if not majority risk
3. **Precautionary principle:** Even 1% existential risk demands action

---

## Implications by Estimate Level

### If P(doom) = 0-1%
- **Action:** Minimal dedicated resources, monitor developments
- **Policy:** Voluntary industry guidelines sufficient
- **Research priority:** Capabilities can proceed at full speed

### If P(doom) = 5-15% (Current Median)
- **Action:** Serious investment in AI safety (billions/year)
- **Policy:** International coordination, safety standards
- **Research priority:** Balance capabilities and safety

### If P(doom) = 30-50% (Safety Researcher Mean)
- **Action:** Pause or heavily regulate frontier AI development
- **Policy:** Mandatory safety evaluations, international treaty
- **Research priority:** Safety work vastly exceeds capabilities

### If P(doom) = >80% (Pessimist View)
- **Action:** Immediate halt to all frontier AI work
- **Policy:** Global moratorium, potential civilizational pivot
- **Research priority:** Only alignment work, no capabilities

---

## Notable Quotes

### High P(doom)
> "The most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die."
> — Eliezer Yudkowsky

> "AI will definitely kill everyone... within 100 years."
> — Roman Yampolskiy

### Moderate P(doom)
> "There's a 25% chance that things go really, really badly."
> — Dario Amodei (Anthropic CEO)

> "I think there's greater than a 10% chance this leads to something terrible."
> — Geoffrey Hinton

### Low P(doom)
> "P(doom) is less than the probability of an asteroid wiping us out."
> — Yann LeCun

> "The probability is equivalent to all the oxygen in my room spontaneously moving to a corner thereby suffocating me."
> — Grady Booch

---

## Summary Statistics

- **Range:** 0% to 99.999999%
- **Median (2023 survey):** 5%
- **Mean (2023 survey):** 14.4%
- **Mode:** 10-20% range (plurality of estimates)
- **Safety researchers mean:** 30%
- **Superforecasters:** 0.38%

**Temporal trend:** Increasing over time as AI capabilities advance

**Key insight:** Even the median estimate (5-14%) represents a non-trivial existential risk that would justify substantial precautionary investment.

---

**Sources:**
- Wikipedia: P(doom) article
- PauseAI.info: P(doom) estimates compilation
- AI Impacts 2023 Survey
- Individual interviews, podcasts, and publications
